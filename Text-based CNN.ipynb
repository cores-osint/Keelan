{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(self):\n",
    "    # Reads the raw csv file and split into\n",
    "    #sentences (x) and target (y)\n",
    "    df = pd.read_csv(self.data)\n",
    "    df.drop(['id','keyword','location'], axis=1, inplace=True)\n",
    "    \n",
    "    self.x_raw = df['text'].values\n",
    "    self.y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(self):\n",
    "    #Removes special symbols and just keep words in lower and upper form\n",
    "    \n",
    "    self.x_raw = [x.lower() for x in self.x_raw]\n",
    "    self.x_raw = [re.sub(r'[^A-Za-z]+', '', x) for x in self.x_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenization(self):\n",
    "    #Tokenizes each sentence by implementing the nltk tool\n",
    "    self.x_raw = [word_tokenize(x) for x in self.x_raw]\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "    #Builds the vocabulary and keeps the \"x\" most frequent words\n",
    "        self.vocabulary = dict()\n",
    "    \n",
    "    fdist = nltk.FreqDist()\n",
    "    for sentence in self.X_raw:\n",
    "        for word in sentence:\n",
    "            fdist[word] += 1\n",
    "        \n",
    "    common_woords = fdist.most_common(self.num_words)\n",
    "    \n",
    "    for idx, word in enumerate(common_words):\n",
    "        self.vocabulary[word[0]] = (idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Until now each tweet is already tokenized, however we need to \n",
    "#transform each word token into a numeric format, therefore we will \n",
    "#use the dictionary generated in Code 3 to transform each word into \n",
    "#its representation based on indexes.\n",
    "\n",
    "def word_to_idx(self):\n",
    "    # By using the dictionary (vocabulary), it is transformed\n",
    "    #each token into its index based representation\n",
    "    \n",
    "    self.x_tokenized = list()\n",
    "    \n",
    "    for sentence in self.x_raw:\n",
    "        temp_sentence = list()\n",
    "        for word in sentence:\n",
    "            if word in self.vocabulary.keys():\n",
    "                temp_sentence.append(self.vocabulary[word])\n",
    "            self.x_tokenized.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding is introduced IOT standardize the length of each tweet\n",
    "\n",
    "def padding_sentences(self):\n",
    "    # Each sentence which does not fulfill the required length\n",
    "    #is padded with the index 0\n",
    "    \n",
    "    pad_idx = 0\n",
    "    self.x_padded = list()\n",
    "    \n",
    "    for sentence in self.x_tokenized:\n",
    "        while len(sentence) < self.seq_len:\n",
    "            sentence.insert(len(sentence), pad_idx)\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "            self.x_padded = np.array(self.x_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test data\n",
    "\n",
    "def split_data(self):\n",
    "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_padded, self.y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing: \n",
    "    def _init_(self, num_words, seq_len):\n",
    "        self.data = 'data/tweets.csv'\n",
    "        self.num_words = num_words\n",
    "        self.seq_len = seq_len\n",
    "        self.vocabulary = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.ModuleList):\n",
    "    \n",
    "    def _init_(self,params):\n",
    "        super(TextClassifier,self)._init_()\n",
    "        \n",
    "        #parameters regarding text preprocessing\n",
    "        self.seq_len = params.seq_len\n",
    "        self.num_words = params.num_words\n",
    "        self.embedding_size = params.embedding_size\n",
    "        \n",
    "        #dropout definition\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # CNN parameters definition\n",
    "        # Kernel sizes\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "      \n",
    "        # Output size for each convolution\n",
    "        self.out_size = params.out_size\n",
    "        # Number of strides for each convolution\n",
    "        self.stride = params.stride\n",
    "      \n",
    "        # Embedding layer definition\n",
    "        self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "      \n",
    "        # Convolution layers definition\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "      \n",
    "        # Max pooling layers definition\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "      \n",
    "        # Fully connected layer definition\n",
    "        self.fc = nn.Linear(self.in_features_fc(), 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_features_fc(self):\n",
    "\n",
    "    Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "    Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "    \n",
    "    # Calculate size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "    out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "    out_conv_1 = math.floor(out_conv_1)\n",
    "    out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "    out_pool_1 = math.floor(out_pool_1)\n",
    "    \n",
    "     #Calculate size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "    out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "    out_conv_2 = math.floor(out_conv_2)\n",
    "    out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "    out_pool_2 = math.floor(out_pool_2)\n",
    "    \n",
    "    #Calculate size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "    out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "    out_conv_3 = math.floor(out_conv_3)\n",
    "    out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "    out_pool_3 = math.floor(out_pool_3)\n",
    "    \n",
    "    # Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "    out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "    out_conv_4 = math.floor(out_conv_4)\n",
    "    out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "    out_pool_4 = math.floor(out_pool_4)\n",
    "    \n",
    "    # Returns \"flattened\" vector (input for fully connected layer)\n",
    "    return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The forward function will take the vector of tokenized words and pass them\n",
    "#through the embedding layer. Each embedding sentence will be passed through\n",
    "#each of the convolutional and max pooling layers, finally the resulting\n",
    "#vectors will be concatenated and reduced to be introduced to the linear\n",
    "#layer\n",
    "\n",
    "def forward(self,x):\n",
    "    #Sequence of tokes is filtered through an embedding layer\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    #convolution layer 1 is applied\n",
    "    x1 = self.conv_1(x)\n",
    "    x1 = torch.relu(x1)\n",
    "    x1 = self.pool_1(x1)\n",
    "    \n",
    "    #convolution later 2 is applied\n",
    "    x2 = self.conv_2(x)\n",
    "    x2 = torch.relu((x2))\n",
    "    x2 = self.pool_2(x2)\n",
    "    \n",
    "    #convolution layer 3 is applied\n",
    "    x3 = self.conv_3(x)\n",
    "    x3 = torch.relu(x3)\n",
    "    x3 = self.pool_3(x3)\n",
    "    \n",
    "    #Convolution layer 4 is applied\n",
    "    x4 = self.conv_4(x)\n",
    "    x4 = torch.relu(x4)\n",
    "    x4 = self.pool_4(x4)\n",
    "    \n",
    "    ## The output of each convolutional layer is concatenated into a unique vector\n",
    "    union = torch.cat((x1, x2, x3, x4), 2)\n",
    "    union = union.reshape(union.size(0), -1)\n",
    "\n",
    "    # The \"flattened\" vector is passed through a fully connected layer\n",
    "    out = self.fc(union)\n",
    "    # Dropout is applied\t\t\n",
    "    out = self.dropout(out)\n",
    "    # Activation function is applied\n",
    "    out = torch.sigmoid(out)\n",
    "    \n",
    "    return out.squeeze()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "    \n",
    "    def _init_(self, x, y): \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def _len_(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def _getitem_(self,idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, params):\n",
    "\n",
    "\n",
    "#Initialize dataset maper\n",
    "    train = DatasetMaper(data['x_train'], data['y_train'])\n",
    "    test = DatasetMaper(data['x_test'], data['y_test'])\n",
    "\n",
    "#Initialize loaders\n",
    "    loader_train = DataLoader(train, batch_size=params.batch_size)\n",
    "    loader_test = DataLoader(test, batch_size=params.batch_size)\n",
    "\n",
    "#Define optimizer\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=params.learning_rate)\n",
    "    \n",
    "#Start training phase\n",
    "\n",
    "    for epoch in range(params.epochs):\n",
    "    #set model in training model\n",
    "        model.train()\n",
    "        predictions = []\n",
    "    #starts batch training\n",
    "        for x_batch, y_batch in loader_train:\n",
    "        \n",
    "            y_batch = y_batch.type(torch.FloatTensor)\n",
    "        \n",
    "        #Feed the model\n",
    "            y_pred = model(x_batch)\n",
    "        \n",
    "        #loss calculation\n",
    "            loss = F.binary_cross_entropy(y_pred, y_batch)\n",
    "        \n",
    "        #Clean gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        #Gradients Calculation\n",
    "            loss.backward()\n",
    "        \n",
    "        #Gradients upadte\n",
    "            optimizer.step()\n",
    "        \n",
    "        #Save predictions\n",
    "            predictions += list(y_pred.detach().nump())\n",
    "        \n",
    "        #evaluation phase\n",
    "            test_predictions = Run.evaluation(model, loader_test)\n",
    "        \n",
    "        #Metrics calculation\n",
    "            train_accuracy = Run.calculate_accuracy(data['y_train'], predictions)\n",
    "            test_accuracy = Run.calculate_accuracy(data['y_test'], test_predictions)\n",
    "            print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f,\" % (epoch+1, loss.item(), train_accuary, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(grand_truth, predictions):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    \n",
    "    #Gets frequency  of true positives and true negatives\n",
    "    #The threshold is 0.5\n",
    "    \n",
    "    for true, pred in zip(grand_truth, predictions):\n",
    "        if (pred >= 0.5) and (true == 1):\n",
    "            true_positives += 1\n",
    "        elif (pred < 0.5) and (true == 0):\n",
    "            true_negatives += 1 \n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "        #return accuracy\n",
    "        return (true_positives+true_negatives) / len(grand_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
